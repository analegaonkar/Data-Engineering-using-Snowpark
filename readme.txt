Learning - 
Setup Environment: Use stages and tables to ingest and organize raw data from S3 into Snowflake
Snowflake Marketplace: Download the data you need from Snowflake Marketplace and use it for your analysis
Data Engineering: Leverage Snowpark for Python DataFrames to perform data transformations such as group by, aggregate, and join to prep for the data for downstream applications
Orchestrating Pipelines: Use Snowflake Python Tasks API to turn your data pipeline code into operational pipelines with integrated monitoring


What we will do -
Ingest data from an external stage - S3 bucket into a Snowflake table
Access data from Snowflake Marketplace and use it for analysis
Analyze data and perform data engineering tasks using Snowpark DataFrame API, Python Stored Procedures
Create Snowflake Tasks and use the Python Tasks API to schedule data pipelines